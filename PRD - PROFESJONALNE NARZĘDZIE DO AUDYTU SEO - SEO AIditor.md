# PROFESJONALNE NARZƒòDZIE DO AUDYTU SEO

Stw√≥rz zaawansowanƒÖ aplikacjƒô React do kompleksowego audytu technicznego SEO z profesjonalnƒÖ prezentacjƒÖ wynik√≥w, przeznaczonƒÖ dla specjalist√≥w SEO do wykorzystania w procesie sprzeda≈ºowym i konsultingowym.

---
## üéØ NAZWA APLIKACJI - SEO AIditor


## üéØ ARCHITEKTURA I ZA≈ÅO≈ªENIA TECHNICZNE

### Stack technologiczny:
- **React** z Hooks dla zarzƒÖdzania stanem
- **Recharts** lub **Chart.js** do wizualizacji danych
- **Tailwind CSS** dla responsywnego designu
- **Lucide React** dla ikonek
- Asynchroniczne wywo≈Çania API z obs≈ÇugƒÖ b≈Çƒôd√≥w
- localStorage dla cache'owania wynik√≥w

### Struktura aplikacji:
1. Strona g≈Ç√≥wna z formularzem
2. Panel audytu z progress tracking
3. Dashboard wynik√≥w z interaktywnymi sekcjami
4. Generator raport√≥w PDF/prezentacyjnych

---

## üìä SZCZEG√ì≈ÅOWE KATEGORIE AUDYTU

### **1. FUNDAMENTY TECHNICZNE (20% wagi ko≈Ñcowej)**

#### A) Dostƒôpno≈õƒá i wydajno≈õƒá:
- **Status HTTP** (200, 301, 302, 404, 500, 503)
  - Sprawd≈∫ czas odpowiedzi serwera (TTFB < 600ms = 100%, 600-1200ms = 50%, >1200ms = 0%)
  - Test dostƒôpno≈õci z r√≥≈ºnych lokalizacji (symulacja)
  
#### B) Core Web Vitals (Google PageSpeed Insights API):
- **LCP (Largest Contentful Paint)**: < 2.5s = 100 pkt, 2.5-4s = 50 pkt, >4s = 0 pkt
- **FID (First Input Delay)**: < 100ms = 100 pkt, 100-300ms = 50 pkt, >300ms = 0 pkt
- **CLS (Cumulative Layout Shift)**: < 0.1 = 100 pkt, 0.1-0.25 = 50 pkt, >0.25 = 0 pkt
- **INP (Interaction to Next Paint)**: nowa metryka, < 200ms = optimal
- **TTFB (Time to First Byte)**: < 800ms = dobry

#### C) Mobilno≈õƒá i responsywno≈õƒá:
- Test viewport meta tag
- Touch elements sizing (minimum 48x48px)
- Font size readability (minimum 16px dla body)
- Mobile-friendly test (symulacja r√≥≈ºnych rozdzielczo≈õci)
- Sprawd≈∫ czy strona u≈ºywa AMP (bonus points)

#### D) Bezpiecze≈Ñstwo:
- **SSL/HTTPS**: certyfikat wa≈ºny, brak mixed content
- **Security headers**: 
  - Content-Security-Policy
  - X-Frame-Options
  - X-Content-Type-Options
  - Strict-Transport-Security
- Test przekierowa≈Ñ HTTP‚ÜíHTTPS
- Sprawd≈∫ wersjƒô TLS (minimum 1.2)

---

### **2. ELEMENTY ON-PAGE (25% wagi ko≈Ñcowej)**

#### A) Meta tagi - szczeg√≥≈Çowa analiza:
- **Title Tag**:
  - D≈Çugo≈õƒá: 50-60 znak√≥w = 100%, 30-50 lub 60-70 = 70%, <30 lub >70 = 30%
  - Czy zawiera brand?
  - Czy jest unikalny? (sprawd≈∫ czy nie duplikuje siƒô z innymi stronami)
  - Pozycja s≈Ç√≥w kluczowych (im wcze≈õniej tym lepiej)
  - Sprawd≈∫ czy nie jest przeklikowy (brak nadmiernych znak√≥wspecjalnych)

- **Meta Description**:
  - D≈Çugo≈õƒá: 150-160 znak√≥w = optimal
  - Czy zawiera Call-to-Action?
  - Czy jest unikalny?
  - Sprawd≈∫ keyword matching
  - Ocena atrakcyjno≈õci (punkty za: pytania, liczby, emocje)

- **Open Graph tags** (Facebook/social):
  - og:title, og:description, og:image, og:type
  - Twitter Card tags
  - Rozmiar i format obraz√≥w OG (1200x630px optimal)

#### B) Struktura nag≈Ç√≥wk√≥w:
- **Hierarchia H1-H6**:
  - Tylko jeden H1 na stronie = 100%, brak H1 = 0%, wiƒôcej ni≈º jeden H1 = 50%
  - Czy H2-H6 sƒÖ w logicznej kolejno≈õci?
  - D≈Çugo≈õƒá nag≈Ç√≥wk√≥w (optimal 20-70 znak√≥w)
  - Keyword optimization w H1 i H2
  - Sprawd≈∫ czy nag≈Ç√≥wki sƒÖ semantycznie poprawne (nie u≈ºywane tylko dla stylowania)

#### C) Obrazy i multimedia:
- **Alt teksty**:
  - % obraz√≥w z alt = score
  - D≈Çugo≈õƒá alt (5-125 znak√≥w optimal)
  - Czy alt nie duplikuje dok≈Çadnie title?
  - Sprawd≈∫ czy alt nie jest keyword stuffing
  
- **Optymalizacja obraz√≥w**:
  - Format (WebP > PNG > JPG dla web)
  - Rozmiar plik√≥w (<100KB dla obraz√≥w hero optimal)
  - Lazy loading implementation
  - Responsive images (srcset, sizes)
  - Width/height attributes (prevent CLS)

#### D) Linki:
- **Linki wewnƒôtrzne**:
  - Ilo≈õƒá link√≥w wewnƒôtrznych (10-50 = optimal)
  - Anchor text diversity
  - Sprawd≈∫ broken links (404)
  - G≈Çƒôboko≈õƒá linkowania (max 3 klikniƒôcia od homepage)
  - Czy sƒÖ linki do wa≈ºnych podstron?

- **Linki zewnƒôtrzne**:
  - Ilo≈õƒá i jako≈õƒá outbound links
  - Czy sƒÖ rel="nofollow" tam gdzie powinny?
  - Sprawd≈∫ czy linki prowadzƒÖ do wiarygodnych ≈∫r√≥de≈Ç
  - Test dostƒôpno≈õci linkowanych stron

---

### **3. INDEKSOWANIE I CRAWLABILITY (20% wagi ko≈Ñcowej)**

#### A) Robots.txt:
- Czy istnieje? (/robots.txt)
- Czy jest poprawnie sformatowany?
- Sprawd≈∫ czy nie blokuje wa≈ºnych zasob√≥w (CSS, JS, obrazy)
- Czy wskazuje na sitemap?
- Sprawd≈∫ dyrektywy User-agent
- Test czy nie blokuje ca≈Çej strony przypadkowo

#### B) Sitemap.xml:
- Czy istnieje? (sprawd≈∫ /sitemap.xml, /sitemap_index.xml)
- Czy jest zg≈Çoszony w robots.txt?
- Ilo≈õƒá URL-i w sitemap (max 50,000)
- Czy wszystkie URL zwracajƒÖ 200?
- Sprawd≈∫ daty ostatniej modyfikacji
- Priority i changefreq values
- Czy jest w formacie XML (nie HTML)?

#### C) Canonical i indeksowanie:
- **Canonical tags**:
  - Czy ka≈ºda strona ma canonical?
  - Czy canonical wskazuje na siebie lub w≈Ça≈õciwy URL?
  - Sprawd≈∫ czy nie ma canonical chains
  - Self-referencing canonical = best practice

- **Meta robots**:
  - Sprawd≈∫ dyrektywy index/noindex
  - follow/nofollow
  - Czy sƒÖ konflikty z robots.txt?

- **Pagination**:
  - rel="next" i rel="prev" (je≈õli dotyczy)
  - Sprawd≈∫ parametry URL (?page=2)

#### D) Schema Markup (Structured Data):
- Czy strona u≈ºywa Schema.org?
- Typy schema: Organization, LocalBusiness, Product, Article, BreadcrumbList, FAQ, Review
- Sprawd≈∫ czy schema jest valid (JSON-LD preferred)
- Rich snippets potential (ocena potencja≈Çu do wy≈õwietlenia)
- Open Graph i Twitter Cards compliance

---

### **4. CONTENT QUALITY & UX (20% wagi ko≈Ñcowej)**

#### A) Analiza tre≈õci:
- **D≈Çugo≈õƒá contentu**:
  - Word count: <300 = thin content (20%), 300-600 = acceptable (60%), 600-1500 = good (90%), >1500 = excellent (100%)
  - Text to HTML ratio (>15% = dobry)
  - Sprawd≈∫ czy content nie jest AI-generated (podstawowe wska≈∫niki)

- **Keyword optimization**:
  - Keyword density (1-3% = optimal, >5% = keyword stuffing)
  - LSI keywords presence (synonimy, powiƒÖzane terminy)
  - Keyword w pierwszym paragrafie
  - Keyword distribution (r√≥wnomierne rozmieszczenie)

- **Readability**:
  - D≈Çugo≈õƒá zda≈Ñ (average <20 s≈Ç√≥w = dobra)
  - D≈Çugo≈õƒá paragraf√≥w (<150 s≈Ç√≥w)
  - U≈ºycie list i bullet points
  - Bia≈Çe przestrzenie
  - Gunning Fog Index lub Flesch Reading Ease

#### B) User Experience:
- **Nawigacja**:
  - Menu g≈Ç√≥wne (czytelne, max 7 pozycji)
  - Breadcrumbs
  - Search functionality
  - Footer links

- **CTA (Call-to-Action)**:
  - Czy sƒÖ widoczne CTA?
  - Ilo≈õƒá CTA (1-3 na stronƒô optimal)
  - Kontrast i visibility

- **Pop-ups i interstitials**:
  - Czy sƒÖ intrusive pop-ups? (Google penalty risk)
  - Timing pop-up√≥w (nie natychmiast)

#### C) Multimedia i engagement:
- Obecno≈õƒá wideo (bonus points)
- Infografiki
- Interaktywne elementy
- Social sharing buttons
- Komentarze/recenzje

---

### **5. TECHNICAL SEO ADVANCED (15% wagi ko≈Ñcowej)**

#### A) Architektura URL:
- **Struktura**:
  - D≈Çugo≈õƒá URL (<75 znak√≥w optimal)
  - Czytelno≈õƒá (human-readable)
  - Separator (dash vs underscore - dash preferred)
  - Brak parametr√≥w dynamicznych (query strings minimalizowane)
  - Lowercase vs uppercase (lowercase preferred)
  - SSL (https://)

- **URL patterns**:
  - Sprawd≈∫ trailing slash consistency
  - Duplikaty URL (www vs non-www, index.html vs /)
  - Sprawd≈∫ redirects (301 vs 302)

#### B) International SEO:
- **Hreflang tags** (je≈õli multi-language):
  - Poprawno≈õƒá implementacji
  - Return tags
  - x-default version

- **Language detection**:
  - HTML lang attribute
  - Content-Language header

#### C) JavaScript i renderowanie:
- Czy strona u≈ºywa client-side rendering (CSR) czy SSR/SSG?
- Test renderowania JavaScript (czy content jest dostƒôpny bez JS?)
- Hydration issues
- Sprawd≈∫ lazy loading implementation

#### D) Paginacja i duplicates:
- Duplicate content detection (basic check)
- Parameter handling
- Sprawd≈∫ czy sƒÖ soft 404

---

## üé® SYSTEM SCORINGU I PREZENTACJI WYNIK√ìW

### Scoring Algorithm:

```
WYNIK KO≈ÉCOWY = 
  (Fundamenty Techniczne √ó 0.20) +
  (On-Page √ó 0.25) +
  (Indeksowanie √ó 0.20) +
  (Content & UX √ó 0.20) +
  (Technical Advanced √ó 0.15)

Klasyfikacja:
90-100: EXCELLENT (zielony) üü¢
75-89: GOOD (jasnozielony) üü°
60-74: NEEDS IMPROVEMENT (≈º√≥≈Çty) üü†
40-59: POOR (pomara≈Ñczowy) üî¥
0-39: CRITICAL (ciemnoczerwony) ‚õî
```

### Kategoryzacja problem√≥w:

#### üî¥ KRYTYCZNE (Impact Score: 9-10):
- Brak HTTPS
- Strona nie indeksuje siƒô (noindex)
- Brak title lub H1
- Czas ≈Çadowania >5s
- Broken strona g≈Ç√≥wna
- Core Web Vitals - wszystkie failed

**Format prezentacji**:
```
‚ùå PROBLEM: Brak certyfikatu SSL
üìä IMPACT: 10/10 - Krytyczny wp≈Çyw na ranking i zaufanie
‚è±Ô∏è CZAS NAPRAWY: 2-4 godziny
üí∞ WP≈ÅYW NA BIZNES: -40% ruchu organicznego, utrata zaufania klient√≥w
‚úÖ ROZWIƒÑZANIE: Zakup i instalacja certyfikatu SSL, przekierowanie HTTP‚ÜíHTTPS
üíµ KOSZT: 0-500 PLN rocznie
```

#### üü° WA≈ªNE (Impact Score: 6-8):
- S≈Çaba meta description
- Brak alt w obrazach
- D≈Çugi czas ≈Çadowania (2-5s)
- Brak schema markup
- Problemy z mobile

#### üü¢ ZALECENIA (Impact Score: 1-5):
- Optymalizacja keyword density
- Dodanie Open Graph tags
- Poprawa readability
- Dodatkowe linki wewnƒôtrzne

---

## üìà DASHBOARD WYNIK√ìW - SZCZEG√ì≈ÅY

### 1. HERO SECTION:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   OG√ìLNY WYNIK SEO HEALTH           ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ         ‚îÇ   78    ‚îÇ  Wykres ko≈Çowy  ‚îÇ
‚îÇ         ‚îÇ  /100   ‚îÇ  z gradientem   ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  koloru         ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ   GOOD - Needs improvement          ‚îÇ
‚îÇ   üü° Wymaga uwagi w 12 obszarach   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. KATEGORIE - Rozwijane sekcje:

**Ka≈ºda kategoria pokazuje**:
- Progress bar z wynikiem
- Liczba problem√≥w (ikona alert)
- Rozwijalna lista szczeg√≥≈Ç√≥w
- Quick fix recommendations

**Przyk≈Çad wizualizacji**:
```
üîß FUNDAMENTY TECHNICZNE: 85/100 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå‚ñë
   ‚úÖ 4 elementy OK | ‚ö†Ô∏è 2 do poprawy | ‚ùå 1 krytyczny
   
   [Rozwi≈Ñ szczeg√≥≈Çy ‚ñº]
   
   ‚Üí Core Web Vitals
     LCP: 2.3s ‚úÖ | FID: 45ms ‚úÖ | CLS: 0.08 ‚úÖ
   
   ‚Üí Bezpiecze≈Ñstwo
     ‚ö†Ô∏è Brak niekt√≥rych security headers
```

### 3. SEKCJA "QUICK WINS" - Priorytetowa:

```
‚ö° SZYBKIE WYGRANE (High Impact, Low Effort)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Dodaj brakujƒÖce alt teksty         ‚îÇ
‚îÇ    Impact: 8/10 | Czas: 30 min        ‚îÇ
‚îÇ    Potencja≈Ç: +15% click-through      ‚îÇ
‚îÇ    ‚úì 23 obrazy bez alt wykryte        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Wyd≈Çu≈º meta description             ‚îÇ
‚îÇ    Impact: 7/10 | Czas: 15 min        ‚îÇ
‚îÇ    Potencja≈Ç: +20% CTR w SERP         ‚îÇ
‚îÇ    ‚úì Obecnie 89 znak√≥w, dodaj 60     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4. ANALIZA KONKURENCJI:

**Por√≥wnanie head-to-head**:
```
           ‚îÇ  TWOJA  ‚îÇ  Konkurent 1  ‚îÇ  Konkurent 2  ‚îÇ LIDER
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Wynik SEO  ‚îÇ   78    ‚îÇ      82       ‚îÇ      91       ‚îÇ  95
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Load Time  ‚îÇ  3.2s   ‚îÇ     2.1s      ‚îÇ     1.8s      ‚îÇ 1.5s
S≈Ç√≥w       ‚îÇ   450   ‚îÇ      890      ‚îÇ     1,200     ‚îÇ 1,500
Backlinks  ‚îÇ   12    ‚îÇ      45       ‚îÇ      120      ‚îÇ  340
Schema     ‚îÇ   ‚ùå    ‚îÇ      ‚úÖ       ‚îÇ      ‚úÖ       ‚îÇ  ‚úÖ

üìä Jeste≈õ 17 punkt√≥w za liderem bran≈ºy
üéØ Najwiƒôksza luka: Content length i technical optimization
```

### 5. BUSINESS IMPACT CALCULATOR:

```
üíº CO TO ZNACZY DLA TWOJEGO BIZNESU?

Obecny ruch organiczny: ~500 odwiedzin/miesiƒÖc (estymacja)

Po naprawie problem√≥w KRYTYCZNYCH:
  ‚ûú Potencjalny wzrost: +60-80%
  ‚ûú Estymowany ruch: ~800-900 odwiedzin/miesiƒÖc
  ‚ûú Przy konwersji 2%: +6-8 lead√≥w/miesiƒÖc
  ‚ûú Warto≈õƒá: ~3,000-4,000 PLN dodatkowego przychodu*

Po implementacji WSZYSTKICH rekomendacji:
  ‚ûú Potencjalny wzrost: +150-200%
  ‚ûú Estymowany ruch: ~1,250-1,500 odwiedzin/miesiƒÖc
  ‚ûú Przy konwersji 2%: +15-20 lead√≥w/miesiƒÖc
  ‚ûú Warto≈õƒá: ~7,500-10,000 PLN dodatkowego przychodu*

*Za≈Ço≈ºenia: ≈õrednia warto≈õƒá leada 500 PLN
Czas implementacji: 3-6 miesiƒôcy
ROI: 300-400%

[SLIDER: Dostosuj warto≈õƒá leada do swoich szacunk√≥w]
```

---

## üìÑ GENERATOR RAPORT√ìW

### Format PDF z sekcjami:

**EXECUTIVE SUMMARY (1 strona)**:
- Og√≥lny wynik SEO Health Score
- Top 5 problem√≥w krytycznych
- Potencja≈Ç wzrostu (liczby)
- Rekomendowany bud≈ºet i timeline

**TECHNICAL DEEP DIVE (2-3 strony)**:
- Wszystkie kategorie z wynikami
- Szczeg√≥≈Çowe problemy z rozwiƒÖzaniami
- Tabele i wykresy

**ACTION PLAN (1 strona)**:
```
PLAN DZIA≈ÅANIA - PRIORYTETYZACJA

FAZA 1 (MiesiƒÖc 1-2): FUNDAMENTY - Bud≈ºet: 3,000-5,000 PLN
‚úì Naprawa problem√≥w krytycznych
‚úì Optymalizacja Core Web Vitals
‚úì Implementacja SSL i security

FAZA 2 (MiesiƒÖc 3-4): OPTYMALIZACJA - Bud≈ºet: 4,000-6,000 PLN
‚úì Przepisanie meta tag√≥w
‚úì Optymalizacja obraz√≥w
‚úì Dodanie schema markup

FAZA 3 (MiesiƒÖc 5-6): CONTENT & GROWTH - Bud≈ºet: 5,000-8,000 PLN
‚úì Rozbudowa tre≈õci
‚úì Link building (podstawy)
‚úì Monitoring i dostrajanie
```

**COMPETITIVE ANALYSIS (1 strona)**:
- Twoja pozycja vs top 3 konkurent√≥w
- Gap analysis
- Opportunities

---

## üõ†Ô∏è FUNKCJE DODATKOWE

### 1. **Historical Tracking**:
- Zapisz audyt w localStorage
- Por√≥wnaj wyniki z poprzednimi audytami
- Poka≈º progress timeline
- Wykres trend√≥w

### 2. **Custom Branding**:
```javascript
const brandingConfig = {
  agencyName: "Twoja Agencja SEO",
  logo: "url_to_logo.png",
  primaryColor: "#FF6B35",
  contactInfo: {
    email: "kontakt@agencja.pl",
    phone: "+48 123 456 789"
  }
}
```

### 3. **Pricing Calculator**:
```
WYCENA US≈ÅUG NA PODSTAWIE AUDYTU:

Wykryte problemy: 27
- Krytyczne: 5 √ó 800 PLN = 4,000 PLN
- Wa≈ºne: 12 √ó 400 PLN = 4,800 PLN
- Zalecenia: 10 √ó 200 PLN = 2,000 PLN

Pakiet BASIC (tylko krytyczne): 4,000 PLN
Pakiet STANDARD (+ wa≈ºne): 8,800 PLN
Pakiet PREMIUM (wszystko + monitoring): 12,500 PLN

[Mo≈ºliwo≈õƒá edycji stawek]
```

### 4. **Email Report Sender**:
- Wygeneruj unique link do raportu
- Wy≈õlij email z podsumowaniem
- Scheduled follow-up reminders

### 5. **Export Options**:
- **PDF** - pe≈Çny raport brandowany
- **CSV** - lista problem√≥w do Excel
- **JSON** - dane surowe dla integracji
- **Prezentacja** - slajdy do klienta (PowerPoint style w HTML)

---

## üí° ADVANCED FEATURES (dla wyr√≥≈ºnienia)

### 1. **AI-Powered Insights**:
U≈ºyj Claude API (poprzez `fetch` w artifact) do:
- Generowania custom rekomendacji na podstawie bran≈ºy
- Analiza konkurencyjnych keywords
- Sugestie content improvements
- Personalizowane action plans

**Przyk≈Çad implementacji**:
```javascript
// Analiza kontekstu biznesowego
const businessContext = `
Audytowana strona: ${url}
Bran≈ºa: ${industry}
Problemy: ${JSON.stringify(issues)}
`;

// Zapytaj Claude o rekomendacje
const recommendations = await getClaudeRecommendations(businessContext);
```

### 2. **Competitive Keywords Analysis**:
- Scraping meta titles konkurent√≥w
- Analiza najczƒôstszych keywords
- Gap analysis - czego Ci brakuje
- Keyword difficulty estimation

### 3. **Local SEO Module** (je≈õli dotyczy):
- NAP consistency check (Name, Address, Phone)
- Google Business Profile optimization tips
- Local citations
- Schema LocalBusiness

### 4. **Backlink Profile Preview** (basic):
- Estymacja ilo≈õci backlink√≥w (poprzez API lub symulacjƒô)
- Domain Authority score approximation
- Link quality indicators

### 5. **Real-time Monitoring Dashboard**:
- Zak≈Çadaj uptime monitoring (symulacja)
- Speed tracking over time
- Ranking changes (mock data)

---

## üé® UX/UI BEST PRACTICES

### Design System:
```css
Kolory:
- Excellent: #10B981 (zielony)
- Good: #84CC16 (limonkowy)
- Warning: #F59E0B (pomara≈Ñczowy)
- Poor: #EF4444 (czerwony)
- Critical: #DC2626 (ciemnoczerwony)

Typography:
- Headings: Inter, bold
- Body: Inter, regular
- Mono (code): JetBrains Mono

Spacing:
- Mobile-first approach
- 8px grid system
- Generous whitespace

Animations:
- Smooth transitions (300ms ease-in-out)
- Progress bars animated
- Fade-ins dla wynik√≥w
- Skeleton loaders podczas audytu
```

### Responsive Breakpoints:
- Mobile: 320px - 640px
- Tablet: 641px - 1024px
- Desktop: 1025px+

### Accessibility:
- Minimum contrast ratio 4.5:1
- Keyboard navigation
- ARIA labels
- Screen reader friendly
- Focus indicators

---

## üì± USER JOURNEY

### Krok 1: Landing
```
[LOGO AGENCJI]

Przeprowad≈∫ profesjonalny audyt SEO w 60 sekund

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ https://                              ‚îÇ
‚îÇ [Wpisz URL strony do audytu]         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[üöÄ ROZPOCZNIJ DARMOWY AUDYT]

‚úì Bez rejestracji
‚úì Raport PDF gratis
‚úì Szczeg√≥≈Çowa analiza 50+ parametr√≥w
```

### Krok 2: Audyt w toku
```
Analizujƒô TwojƒÖ stronƒô...

‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 60% 

‚úÖ Sprawdzono dostƒôpno≈õƒá
‚úÖ Pobrano HTML
‚úÖ Analiza Core Web Vitals
‚è≥ Sprawdzanie meta tag√≥w...
‚è≥ Analiza struktury...

Pozosta≈Ço oko≈Ço 20 sekund...
```

### Krok 3: Wyniki
```
[Pe≈Çny dashboard jak opisany wy≈ºej]

[CTA: POBIERZ PE≈ÅNY RAPORT PDF]
[CTA: UM√ìW KONSULTACJƒò]
[CTA: POR√ìWNAJ Z KONKURENCJƒÑ]
```

---

## ‚öôÔ∏è IMPLEMENTACJA TECHNICZNA

### API Calls i Data Fetching:

**1. Core check - dostƒôpno≈õƒá**:
```javascript
const checkAvailability = async (url) => {
  try {
    const response = await fetch(url, { method: 'HEAD' });
    return {
      status: response.status,
      ssl: url.startsWith('https://'),
      ttfb: performance.now() // simplified
    };
  } catch (error) {
    return { available: false, error: error.message };
  }
};
```

**2. HTML Parsing**:
```javascript
const analyzeHTML = async (url) => {
  const html = await fetch(url).then(r => r.text());
  const parser = new DOMParser();
  const doc = parser.parseFromString(html, 'text/html');
  
  return {
    title: doc.querySelector('title')?.textContent,
    metaDescription: doc.querySelector('meta[name="description"]')?.content,
    h1: doc.querySelectorAll('h1'),
    images: doc.querySelectorAll('img'),
    links: {
      internal: [...doc.querySelectorAll('a')].filter(a => a.href.includes(domain)),
      external: [...doc.querySelectorAll('a')].filter(a => !a.href.includes(domain))
    },
    // ... wiƒôcej
  };
};
```

**3. PageSpeed Insights**:
```javascript
const API_KEY = 'YOUR_KEY'; // instrukcja jak uzyskaƒá
const analyzeSpeed = async (url) => {
  const endpoint = `https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url=${url}&key=${API_KEY}`;
  const response = await fetch(endpoint);
  const data = await response.json();
  
  return {
    lcp: data.lighthouseResult.audits['largest-contentful-paint'].numericValue,
    fid: data.lighthouseResult.audits['max-potential-fid'].numericValue,
    cls: data.lighthouseResult.audits['cumulative-layout-shift'].numericValue,
    performance: data.lighthouseResult.categories.performance.score * 100
  };
};
```

### Error Handling:
- Try-catch dla wszystkich API calls
- Fallback values je≈õli API fails
- User-friendly error messages
- Retry logic dla timeout√≥w

### Performance Optimization:
- Debouncing URL input
- Async/await dla r√≥wnoleg≈Çych sprawdze≈Ñ
- Cache results in localStorage (24h)
- Lazy loading dla wynik√≥w
- Progressive enhancement

---

## üìö TERMINOLOGIA - TOOLTIPS

Ka≈ºdy termin techniczny powinien mieƒá tooltip z wyja≈õnieniem:

```javascript
const glossary = {
  "Core Web Vitals": "Metryki Google mierzƒÖce do≈õwiadczenie u≈ºytkownika: szybko≈õƒá ≈Çadowania, interaktywno≈õƒá i stabilno≈õƒá wizualnƒÖ",
  "LCP": "Largest Contentful Paint - czas do wy≈õwietlenia g≈Ç√≥wnego elementu strony (obrazek, nag≈Ç√≥wek). Powinien byƒá <2.5s",
  "Meta Description": "Kr√≥tki opis strony wy≈õwietlany w wynikach Google. Wp≈Çywa na CTR ale nie ranking.",
  // ... wszystkie terminy
};
```

---

## ‚úÖ CHECKLIST PRZED WYDANIEM

- [ ] Wszystkie API calls dzia≈ÇajƒÖ z error handling
- [ ] Responsywny design (mobile-first)
- [ ] Loading states i skeleton screens
- [ ] Tooltips dla termin√≥w technicznych
- [ ] PDF generator dzia≈Ça
- [ ] Branding customization dzia≈Ça
- [ ] Results sƒÖ zapisywane w localStorage
- [ ] Accessibility: WCAG 2.1 AA
- [ ] Cross-browser testing (Chrome, Firefox, Safari)
- [ ] Performance: Lighthouse score >90
- [ ] SEO: Meta tags dla samego narzƒôdzia
- [ ] Analytics tracking (opcjonalnie)

---

**DODATKOWE WSKAZ√ìWKI DLA IMPLEMENTACJI**:

1. **Rozpocznij od MVP**: Zaimplementuj podstawowe kategorie audytu, potem dodawaj advanced features

2. **Mock data dla test√≥w**: Przygotuj przyk≈Çadowe wyniki do testowania UI bez rzeczywistych API calls

3. **Progresywna walidacja**: Dodaj walidacjƒô URL przed rozpoczƒôciem audytu

4. **Instrukcje dla u≈ºytkownika**: Dodaj sekcjƒô "Jak korzystaƒá" i FAQ

5. **Legal**: Disclaimer o charakterze informacyjnym wynik√≥w

Aplikacja powinna byƒá **production-ready**, wizualnie **na poziomie SaaS tools**, i dostarczaƒá **actionable insights** kt√≥re mo≈ºna pokazaƒá klientowi podczas spotkania sprzeda≈ºowego.


# ZA≈ÅƒÑCZNIK: IMPLEMENTACJA MVP - WERSJA W 100% DARMOWA

## üéØ ZAKRES MVP (Minimum Viable Product)

### Wdra≈ºane funkcje z g≈Ç√≥wnego planu:

**‚úÖ KATEGORIA 1: FUNDAMENTY TECHNICZNE (20%)**
- Status HTTP i dostƒôpno≈õƒá
- TTFB (Time to First Byte)
- SSL/HTTPS check
- Security headers
- Core Web Vitals via Google PageSpeed Insights API

**‚úÖ KATEGORIA 2: ON-PAGE (25%)**
- Title tag (d≈Çugo≈õƒá, struktura)
- Meta description (d≈Çugo≈õƒá, jako≈õƒá)
- Nag≈Ç√≥wki H1-H6 (hierarchia, ilo≈õƒá)
- Alt teksty w obrazach
- Open Graph tags
- Linki wewnƒôtrzne/zewnƒôtrzne
- Broken links check

**‚úÖ KATEGORIA 3: INDEKSOWANIE (20%)**
- Robots.txt (dostƒôpno≈õƒá, analiza)
- Sitemap.xml (dostƒôpno≈õƒá, analiza)
- Canonical tags
- Meta robots
- Schema markup detection (JSON-LD)

**‚úÖ KATEGORIA 4: CONTENT & UX (20%)**
- Word count
- Keyword density
- Readability score (Flesch Reading Ease)
- Paragraph/sentence length
- Text-to-HTML ratio

**‚úÖ KATEGORIA 5: TECHNICAL ADVANCED (15%)**
- URL structure analysis
- HTML lang attribute
- Viewport meta tag
- Image optimization (sizes, formats)

---

## üîß STACK TECHNICZNY - BIBLIOTEKI

### Python Backend (dla zbierania danych):

```bash
pip install requests beautifulsoup4 lxml textstat validators urllib3
```

**Biblioteki:**
- `requests` - HTTP requests, pobranie HTML, API calls
- `beautifulsoup4` - parsing HTML
- `lxml` - szybszy parser dla BS4
- `textstat` - readability scores
- `validators` - walidacja URL
- `urllib3` - dodatkowe funkcje URL

### Frontend (React):

```bash
# Ju≈º dostƒôpne w artifacts:
- React (z Hooks)
- Recharts (wykresy)
- Tailwind CSS (styling)
- Lucide React (ikony)
```

---

## üìã INSTRUKCJA: GOOGLE PAGESPEED INSIGHTS API

### Krok 1: Utworzenie API Key (5 minut)

1. **Przejd≈∫ do Google Cloud Console:**
   - https://console.cloud.google.com/

2. **Zaloguj siƒô** (konto Gmail)

3. **Utw√≥rz nowy projekt:**
   - Kliknij dropdown przy logo Google Cloud (g√≥ra)
   - "New Project"
   - Nazwa: "SEO Audit Tool"
   - Kliknij "Create"

4. **W≈ÇƒÖcz PageSpeed Insights API:**
   - W menu bocznym: "APIs & Services" > "Library"
   - Wyszukaj: "PageSpeed Insights API"
   - Kliknij kartƒô API
   - Przycisk "ENABLE"

5. **Wygeneruj API Key:**
   - "APIs & Services" > "Credentials"
   - "+ CREATE CREDENTIALS" > "API key"
   - Skopiuj klucz (zapisz bezpiecznie!)

6. **Zabezpiecz klucz (opcjonalnie):**
   - Kliknij na nazwƒô klucza
   - "API restrictions" > wybierz "Restrict key"
   - Zaznacz tylko "PageSpeed Insights API"
   - Save

### Limity darmowe:
- **25,000 zapyta≈Ñ/dzie≈Ñ**
- **Bez koszt√≥w**
- Wystarczy dla ~800 audyt√≥w dziennie (zak≈ÇadajƒÖc 30 request√≥w per audyt)

### U≈ºycie w kodzie:

```python
API_KEY = "YOUR_API_KEY_HERE"
url_to_test = "https://example.com"

endpoint = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url_to_test}&key={API_KEY}&strategy=mobile"

response = requests.get(endpoint)
data = response.json()
```

---

## üíª IMPLEMENTACJA KROK PO KROKU

### STRUKTURA PROJEKTU:

```
seo-audit-tool/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ audit_engine.py          # G≈Ç√≥wna logika audytu
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ technical.py         # Analiza techniczna
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ onpage.py            # Analiza on-page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexing.py          # Robots, sitemap
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content.py           # Content analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pagespeed.py         # Google PSI API
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                 # Helpery
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ SEOAuditApp.jsx          # React artifact
‚îî‚îÄ‚îÄ config.py                     # API keys, settings
```

---

## üìù KOD IMPLEMENTACJI

### 1. **config.py** - Konfiguracja

```python
# config.py

# Google PageSpeed Insights API
GOOGLE_PSI_API_KEY = "YOUR_API_KEY_HERE"  # Wstaw sw√≥j klucz

# Timeouts
REQUEST_TIMEOUT = 10  # sekundy
PSI_TIMEOUT = 30  # PSI mo≈ºe trwaƒá d≈Çu≈ºej

# User agent
USER_AGENT = "SEO-Audit-Tool/1.0 (Educational Purpose)"

# Scoring weights
WEIGHTS = {
    "technical": 0.20,
    "onpage": 0.25,
    "indexing": 0.20,
    "content": 0.20,
    "advanced": 0.15
}

# Thresholds
THRESHOLDS = {
    "title_length_optimal": (50, 60),
    "meta_desc_optimal": (150, 160),
    "page_speed_good": 2500,  # ms
    "word_count_min": 300,
    "word_count_good": 600,
    "keyword_density_optimal": (1, 3)  # %
}
```

### 2. **utils.py** - Funkcje pomocnicze

```python
# utils.py
import requests
import validators
from urllib.parse import urlparse, urljoin
from config import REQUEST_TIMEOUT, USER_AGENT

def validate_url(url):
    """Walidacja i normalizacja URL"""
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    if validators.url(url):
        return url
    return None

def get_domain(url):
    """WyciƒÖgnij domenƒô z URL"""
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}"

def fetch_url(url, timeout=REQUEST_TIMEOUT):
    """Pobierz URL z error handling"""
    try:
        headers = {'User-Agent': USER_AGENT}
        response = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)
        return {
            'success': True,
            'status_code': response.status_code,
            'content': response.text,
            'headers': dict(response.headers),
            'url': response.url,  # final URL po redirectach
            'elapsed': response.elapsed.total_seconds()
        }
    except requests.exceptions.Timeout:
        return {'success': False, 'error': 'Timeout'}
    except requests.exceptions.RequestException as e:
        return {'success': False, 'error': str(e)}

def is_internal_link(link_url, base_domain):
    """Sprawd≈∫ czy link jest wewnƒôtrzny"""
    if not link_url:
        return False
    link_domain = urlparse(link_url).netloc
    base = urlparse(base_domain).netloc
    return link_domain == base or link_domain == ''

def calculate_score(value, min_val, max_val, reverse=False):
    """Oblicz score 0-100 na podstawie warto≈õci"""
    if reverse:
        value = max_val - value + min_val
    
    if value <= min_val:
        return 0
    elif value >= max_val:
        return 100
    else:
        return int(((value - min_val) / (max_val - min_val)) * 100)
```

### 3. **analyzers/technical.py** - Analiza techniczna

```python
# analyzers/technical.py
import requests
from utils import fetch_url, get_domain

def analyze_technical(url, page_data):
    """Analiza fundament√≥w technicznych"""
    results = {
        'score': 0,
        'checks': {},
        'issues': []
    }
    
    # 1. Status HTTP i dostƒôpno≈õƒá
    if page_data['success']:
        results['checks']['http_status'] = {
            'value': page_data['status_code'],
            'pass': page_data['status_code'] == 200,
            'score': 100 if page_data['status_code'] == 200 else 0
        }
        
        if page_data['status_code'] != 200:
            results['issues'].append({
                'severity': 'critical',
                'title': f'Status HTTP: {page_data["status_code"]}',
                'impact': 10,
                'description': 'Strona nie jest dostƒôpna lub zwraca b≈ÇƒÖd'
            })
    else:
        results['checks']['http_status'] = {'pass': False, 'score': 0}
        results['issues'].append({
            'severity': 'critical',
            'title': 'Strona niedostƒôpna',
            'impact': 10,
            'description': page_data.get('error', 'Nie mo≈ºna pobraƒá strony')
        })
        return results
    
    # 2. SSL/HTTPS
    is_https = url.startswith('https://')
    results['checks']['ssl'] = {
        'value': is_https,
        'pass': is_https,
        'score': 100 if is_https else 0
    }
    
    if not is_https:
        results['issues'].append({
            'severity': 'critical',
            'title': 'Brak certyfikatu SSL',
            'impact': 10,
            'description': 'Strona nie u≈ºywa HTTPS - krytyczne dla SEO i bezpiecze≈Ñstwa',
            'fix': 'Zainstaluj certyfikat SSL (darmowy: Let\'s Encrypt) i przekieruj HTTP‚ÜíHTTPS'
        })
    
    # 3. TTFB (Time to First Byte)
    ttfb = page_data['elapsed'] * 1000  # w milisekundach
    results['checks']['ttfb'] = {
        'value': f"{ttfb:.0f}ms",
        'pass': ttfb < 600,
        'score': 100 if ttfb < 600 else (50 if ttfb < 1200 else 0)
    }
    
    if ttfb > 1200:
        results['issues'].append({
            'severity': 'important',
            'title': f'Wolny czas odpowiedzi serwera: {ttfb:.0f}ms',
            'impact': 7,
            'description': 'TTFB powinien byƒá <600ms. Wp≈Çywa na user experience.',
            'fix': 'Optymalizuj serwer, rozwa≈º CDN, w≈ÇƒÖcz caching'
        })
    
    # 4. Security Headers
    headers = page_data.get('headers', {})
    security_headers = {
        'Strict-Transport-Security': 'HSTS',
        'X-Content-Type-Options': 'X-Content-Type',
        'X-Frame-Options': 'X-Frame',
        'Content-Security-Policy': 'CSP'
    }
    
    missing_headers = []
    for header, name in security_headers.items():
        if header not in headers:
            missing_headers.append(name)
    
    results['checks']['security_headers'] = {
        'value': f"{len(security_headers) - len(missing_headers)}/{len(security_headers)}",
        'pass': len(missing_headers) == 0,
        'score': int(((len(security_headers) - len(missing_headers)) / len(security_headers)) * 100)
    }
    
    if missing_headers:
        results['issues'].append({
            'severity': 'recommendation',
            'title': f'Brakuje {len(missing_headers)} security headers',
            'impact': 4,
            'description': f'Brak: {", ".join(missing_headers)}',
            'fix': 'Dodaj security headers w konfiguracji serwera'
        })
    
    # Oblicz ≈õredni score
    scores = [check['score'] for check in results['checks'].values()]
    results['score'] = sum(scores) / len(scores) if scores else 0
    
    return results
```

### 4. **analyzers/onpage.py** - Analiza On-Page

```python
# analyzers/onpage.py
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from config import THRESHOLDS
from utils import is_internal_link, get_domain

def analyze_onpage(url, html_content):
    """Analiza element√≥w on-page"""
    soup = BeautifulSoup(html_content, 'lxml')
    results = {
        'score': 0,
        'checks': {},
        'issues': []
    }
    
    # 1. Title Tag
    title_tag = soup.find('title')
    if title_tag:
        title_text = title_tag.get_text().strip()
        title_length = len(title_text)
        optimal_min, optimal_max = THRESHOLDS['title_length_optimal']
        
        if optimal_min <= title_length <= optimal_max:
            title_score = 100
            title_pass = True
        elif 30 <= title_length < optimal_min or optimal_max < title_length <= 70:
            title_score = 70
            title_pass = False
            results['issues'].append({
                'severity': 'important',
                'title': 'Title tag poza optymalnƒÖ d≈Çugo≈õciƒÖ',
                'impact': 7,
                'description': f'D≈Çugo≈õƒá: {title_length} znak√≥w. Optimal: {optimal_min}-{optimal_max}',
                'fix': f'{"Skr√≥ƒá" if title_length > optimal_max else "Wyd≈Çu≈º"} title do {optimal_min}-{optimal_max} znak√≥w'
            })
        else:
            title_score = 30
            title_pass = False
            results['issues'].append({
                'severity': 'critical',
                'title': 'Title tag zbyt kr√≥tki/d≈Çugi',
                'impact': 9,
                'description': f'D≈Çugo≈õƒá: {title_length} znak√≥w. Drastycznie poza normƒÖ.',
                'fix': f'Przepisz title tag do d≈Çugo≈õci {optimal_min}-{optimal_max} znak√≥w'
            })
        
        results['checks']['title'] = {
            'value': f"{title_length} znak√≥w",
            'text': title_text[:60] + '...' if len(title_text) > 60 else title_text,
            'pass': title_pass,
            'score': title_score
        }
    else:
        results['checks']['title'] = {'pass': False, 'score': 0}
        results['issues'].append({
            'severity': 'critical',
            'title': 'Brak title tag',
            'impact': 10,
            'description': 'Title tag jest fundamentem SEO - musi istnieƒá na ka≈ºdej stronie',
            'fix': 'Dodaj <title>Tw√≥j Tytu≈Ç</title> w <head>'
        })
    
    # 2. Meta Description
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    if meta_desc and meta_desc.get('content'):
        desc_text = meta_desc.get('content').strip()
        desc_length = len(desc_text)
        optimal_min, optimal_max = THRESHOLDS['meta_desc_optimal']
        
        desc_score = 100 if optimal_min <= desc_length <= optimal_max else 50
        results['checks']['meta_description'] = {
            'value': f"{desc_length} znak√≥w",
            'text': desc_text[:80] + '...' if len(desc_text) > 80 else desc_text,
            'pass': optimal_min <= desc_length <= optimal_max,
            'score': desc_score
        }
        
        if desc_length < optimal_min or desc_length > optimal_max:
            results['issues'].append({
                'severity': 'important',
                'title': 'Meta description poza optymalnƒÖ d≈Çugo≈õciƒÖ',
                'impact': 6,
                'description': f'D≈Çugo≈õƒá: {desc_length}. Optimal: {optimal_min}-{optimal_max}',
                'fix': 'Dostosuj d≈Çugo≈õƒá meta description i dodaj CTA'
            })
    else:
        results['checks']['meta_description'] = {'pass': False, 'score': 0}
        results['issues'].append({
            'severity': 'important',
            'title': 'Brak meta description',
            'impact': 7,
            'description': 'Meta description wp≈Çywa na CTR w wynikach wyszukiwania',
            'fix': 'Dodaj <meta name="description" content="...">'
        })
    
    # 3. Nag≈Ç√≥wki H1-H6
    h1_tags = soup.find_all('h1')
    h1_count = len(h1_tags)
    
    if h1_count == 1:
        h1_score = 100
        h1_pass = True
        h1_text = h1_tags[0].get_text().strip()
        results['checks']['h1'] = {
            'value': f"1 H1 (‚úì)",
            'text': h1_text[:60] + '...' if len(h1_text) > 60 else h1_text,
            'pass': True,
            'score': 100
        }
    elif h1_count == 0:
        results['checks']['h1'] = {'value': 'Brak H1', 'pass': False, 'score': 0}
        results['issues'].append({
            'severity': 'critical',
            'title': 'Brak nag≈Ç√≥wka H1',
            'impact': 9,
            'description': 'H1 jest kluczowy dla struktury i SEO',
            'fix': 'Dodaj jeden nag≈Ç√≥wek H1 z g≈Ç√≥wnym tematem strony'
        })
    else:
        results['checks']['h1'] = {
            'value': f"{h1_count} H1 (za du≈ºo)",
            'pass': False,
            'score': 50
        }
        results['issues'].append({
            'severity': 'important',
            'title': f'Wiƒôcej ni≈º jeden H1 ({h1_count})',
            'impact': 6,
            'description': 'Powinna byƒá tylko jedna H1 na stronie',
            'fix': 'Zostaw jednƒÖ H1, resztƒô zamie≈Ñ na H2'
        })
    
    # Hierarchia H2-H6
    heading_structure = {
        'h2': len(soup.find_all('h2')),
        'h3': len(soup.find_all('h3')),
        'h4': len(soup.find_all('h4')),
        'h5': len(soup.find_all('h5')),
        'h6': len(soup.find_all('h6'))
    }
    
    results['checks']['heading_structure'] = {
        'value': f"H2:{heading_structure['h2']}, H3:{heading_structure['h3']}",
        'pass': heading_structure['h2'] > 0,
        'score': 100 if heading_structure['h2'] > 0 else 70
    }
    
    # 4. Obrazy i Alt teksty
    images = soup.find_all('img')
    images_with_alt = [img for img in images if img.get('alt')]
    alt_percentage = (len(images_with_alt) / len(images) * 100) if images else 100
    
    results['checks']['images_alt'] = {
        'value': f"{len(images_with_alt)}/{len(images)} z alt",
        'pass': alt_percentage >= 90,
        'score': int(alt_percentage)
    }
    
    if alt_percentage < 90 and len(images) > 0:
        missing_alt = len(images) - len(images_with_alt)
        results['issues'].append({
            'severity': 'important' if alt_percentage < 50 else 'recommendation',
            'title': f'{missing_alt} obraz√≥w bez alt',
            'impact': 8 if alt_percentage < 50 else 5,
            'description': f'{missing_alt} z {len(images)} obraz√≥w nie ma alt tekstu',
            'fix': 'Dodaj opisowe alt teksty do wszystkich obraz√≥w'
        })
    
    # 5. Open Graph tags
    og_tags = {
        'og:title': soup.find('meta', property='og:title'),
        'og:description': soup.find('meta', property='og:description'),
        'og:image': soup.find('meta', property='og:image'),
        'og:type': soup.find('meta', property='og:type')
    }
    
    og_present = sum(1 for tag in og_tags.values() if tag)
    results['checks']['open_graph'] = {
        'value': f"{og_present}/4 OG tags",
        'pass': og_present >= 3,
        'score': int((og_present / 4) * 100)
    }
    
    if og_present < 3:
        results['issues'].append({
            'severity': 'recommendation',
            'title': 'Niekompletne Open Graph tags',
            'impact': 4,
            'description': f'Brak {4 - og_present} podstawowych OG tag√≥w dla social media',
            'fix': 'Dodaj og:title, og:description, og:image, og:type'
        })
    
    # 6. Linki
    all_links = soup.find_all('a', href=True)
    base_domain = get_domain(url)
    
    internal_links = [link for link in all_links if is_internal_link(link['href'], base_domain)]
    external_links = [link for link in all_links if not is_internal_link(link['href'], base_domain)]
    
    results['checks']['links'] = {
        'value': f"Wew: {len(internal_links)}, Zew: {len(external_links)}",
        'pass': 5 <= len(internal_links) <= 100,
        'score': 100 if 10 <= len(internal_links) <= 50 else 70
    }
    
    if len(internal_links) < 5:
        results['issues'].append({
            'severity': 'recommendation',
            'title': 'Ma≈Ço link√≥w wewnƒôtrznych',
            'impact': 5,
            'description': f'Tylko {len(internal_links)} link√≥w wewnƒôtrznych',
            'fix': 'Dodaj wiƒôcej link√≥w do innych podstron (10-30 optimal)'
        })
    
    # Oblicz ≈õredni score
    scores = [check['score'] for check in results['checks'].values()]
    results['score'] = sum(scores) / len(scores) if scores else 0
    
    return results
```

### 5. **analyzers/indexing.py** - Robots i Sitemap

```python
# analyzers/indexing.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import xml.etree.ElementTree as ET

def analyze_indexing(url, html_content):
    """Analiza crawlability i indexing"""
    results = {
        'score': 0,
        'checks': {},
        'issues': []
    }
    
    soup = BeautifulSoup(html_content, 'lxml')
    base_domain = url.rstrip('/')
    
    # 1. Robots.txt
    robots_url = urljoin(base_domain, '/robots.txt')
    try:
        robots_response = requests.get(robots_url, timeout=5)
        if robots_response.status_code == 200:
            robots_content = robots_response.text
            
            # Sprawd≈∫ czy nie blokuje wszystkiego
            blocks_all = 'Disallow: /' in robots_content and 'User-agent: *' in robots_content
            has_sitemap = 'Sitemap:' in robots_content
            
            results['checks']['robots_txt'] = {
                'value': 'Istnieje ‚úì',
                'pass': not blocks_all,
                'score': 100 if (not blocks_all and has_sitemap) else 70,
                'content_preview': robots_content[:200]
            }
            
            if blocks_all:
                results['issues'].append({
                    'severity': 'critical',
                    'title': 'Robots.txt blokuje ca≈ÇƒÖ stronƒô',
                    'impact': 10,
                    'description': 'User-agent: * + Disallow: / blokuje indeksowanie',
                    'fix': 'NATYCHMIAST usu≈Ñ lub popraw robots.txt!'
                })
            
            if not has_sitemap:
                results['issues'].append({
                    'severity': 'recommendation',
                    'title': 'Robots.txt nie wskazuje sitemap',
                    'impact': 3,
                    'description': 'Brak linku do sitemap.xml w robots.txt',
                    'fix': 'Dodaj liniƒô: Sitemap: https://domain.com/sitemap.xml'
                })
        else:
            results['checks']['robots_txt'] = {
                'value': 'Brak (404)',
                'pass': True,  # Brak robots.txt nie jest b≈Çƒôdem
                'score': 80
            }
            results['issues'].append({
                'severity': 'recommendation',
                'title': 'Brak robots.txt',
                'impact': 2,
                'description': 'Warto utworzyƒá robots.txt dla kontroli crawlingu',
                'fix': 'Utw√≥rz podstawowy robots.txt ze wskazaniem sitemap'
            })
    except:
        results['checks']['robots_txt'] = {'value': 'Error', 'pass': False, 'score': 50}
    
    # 2. Sitemap.xml
    sitemap_url = urljoin(base_domain, '/sitemap.xml')
    try:
        sitemap_response = requests.get(sitemap_url, timeout=5)
        if sitemap_response.status_code == 200:
            # Spr√≥buj sparsowaƒá XML
            try:
                root = ET.fromstring(sitemap_response.content)
                # Policz URLe
                url_count = len(root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'))
                
                results['checks']['sitemap'] = {
                    'value': f'Istnieje ({url_count} URLs)',
                    'pass': True,
                    'score': 100
                }
                
                if url_count > 50000:
                    results['issues'].append({
                        'severity': 'important',
                        'title': 'Sitemap zbyt du≈ºa',
                        'impact': 6,
                        'description': f'{url_count} URLs (max 50,000)',
                        'fix': 'Podziel sitemap na mniejsze pliki (sitemap index)'
                    })
            except:
                results['checks']['sitemap'] = {
                    'value': 'Istnieje (niepoprawny XML)',
                    'pass': False,
                    'score': 50
                }
                results['issues'].append({
                    'severity': 'important',
                    'title': 'Sitemap niepoprawny',
                    'impact': 7,
                    'description': 'Sitemap istnieje ale ma b≈Çƒôdy sk≈Çadni XML',
                    'fix': 'Napraw sk≈Çadniƒô XML w sitemap.xml'
                })
        else:
            results['checks']['sitemap'] = {
                'value': 'Brak (404)',
                'pass': False,
                'score': 0
            }
            results['issues'].append({
                'severity': 'critical',
                'title': 'Brak sitemap.xml',
                'impact': 8,
                'description': 'Sitemap pomaga Google crawlowaƒá stronƒô',
                'fix': 'Wygeneruj i opublikuj sitemap.xml'
            })
    except:
        results['checks']['sitemap'] = {'value': 'Error', 'pass': False, 'score': 0}
    
    # 3. Canonical tag
    canonical = soup.find('link', rel='canonical')
    if canonical and canonical.get('href'):
        canonical_url = canonical.get('href')
        results['checks']['canonical'] = {
            'value': 'Istnieje ‚úì',
            'pass': True,
            'score': 100,
            'url': canonical_url
        }
    else:
        results['checks']['canonical'] = {
            'value': 'Brak',
            'pass': False,
            'score': 60
        }
        results['issues'].append({
            'severity': 'recommendation',
            'title': 'Brak canonical tag',
            'impact': 5,
            'description': 'Canonical zapobiega duplicate content',
            'fix': 'Dodaj <link rel="canonical" href="URL"> w <head>'
        })
    
    # 4. Meta robots
    meta_robots = soup.find('meta', attrs={'name': 'robots'})
    if meta_robots:
        content = meta_robots.get('content', '').lower()
        is_noindex = 'noindex' in content
        
        results['checks']['meta_robots'] = {
            'value': content,
            'pass': not is_noindex,
            'score': 0 if is_noindex else 100
        }
        
        if is_noindex:
            results['issues'].append({
                'severity': 'critical',
                'title': 'Strona ma meta noindex!',
                'impact': 10,
                'description': 'Meta robots="noindex" blokuje indeksowanie',
                'fix': 'NATYCHMIAST usu≈Ñ noindex lub zmie≈Ñ na index'
            })
    else:
        results['checks']['meta_robots'] = {
            'value': 'Brak (index by default)',
            'pass': True,
            'score': 100
        }
    
    # 5. Schema Markup (JSON-LD)
    schema_scripts = soup.find_all('script', type='application/ld+json')
    schema_count = len(schema_scripts)
    
    results['checks']['schema_markup'] = {
        'value': f'{schema_count} schema' if schema_count > 0 else 'Brak',
        'pass': schema_count > 0,
        'score': 100 if schema_count > 0 else 40
    }
    
    if schema_count == 0:
        results['issues'].append({
            'severity': 'recommendation',
            'title': 'Brak Schema Markup',
            'impact': 6,
            'description': 'Schema.org daje rich snippets w Google',
            'fix': 'Dodaj JSON-LD schema (Organization, Article, etc.)'
        })
    
    # Oblicz ≈õredni score
    scores = [check['score'] for check in results['checks'].values()]
    results['score'] = sum(scores) / len(scores) if scores else 0
    
    return results
```

### 6. **analyzers/content.py** - Analiza tre≈õci

```python
# analyzers/content.py
from bs4 import BeautifulSoup
import textstat
import re
from collections import Counter

def analyze_content(html_content):
    """Analiza jako≈õci contentu"""
    results = {
        'score': 0,
        'checks': {},
        'issues': []
    }
    
    soup = BeautifulSoup(html_content, 'lxml')
    
    # Usu≈Ñ script i style z analizy
    for script in soup(['script', 'style', 'nav', 'footer', 'header']):
        script.decompose()
    
    # WyciƒÖgnij tekst
    text = soup.get_text(separator=' ', strip=True)
    text = re.sub(r'\s+', ' ', text)  # Normalizuj bia≈Çe znaki
    
    # 1. Word count
    words = text.split()
    word_count = len(words)
    
    if word_count >= 1500:
        wc_score = 100
        wc_severity = None
    elif word_count >= 600:
        wc_score = 90
        wc_severity = None
    elif word_count >= 300:
        wc_score = 60
        wc_severity = 'recommendation'
    else:
        wc_score = 20
        wc_severity = 'important'
    
    results['checks']['word_count'] = {
        'value': f'{word_count} s≈Ç√≥w',
        'pass': word_count >= 300,
        'score': wc_score
    }
    
    if wc_severity:
        results['issues'].append({
            'severity': wc_severity,
            'title': f'{"Zbyt" if word_count < 300 else "Ma≈Ço"} tre≈õci: {word_count} s≈Ç√≥w',
            'impact': 8 if word_count < 300 else 5,
            'description': f'Optimal: 600-1500+ s≈Ç√≥w. Thin content = s≈Çabe SEO.',
            'fix': 'Rozbuduj content do minimum 600 s≈Ç√≥w warto≈õciowej tre≈õci'
        })
    
    # 2. Text-to-HTML ratio
    html_size = len(html_content)
    text_size = len(text)
    ratio = (text_size / html_size * 100) if html_size > 0 else 0
    
    results['checks']['text_html_ratio'] = {
        'value': f'{ratio:.1f}%',
        'pass': ratio >= 15,
        'score': 100 if ratio >= 15 else int(ratio / 15 * 100)
    }
    
    if ratio < 15:
        results['issues'].append({
            'severity': 'recommendation',
            'title': f'Niski text-to-HTML ratio: {ratio:.1f}%',
            'impact': 4,
            'description': 'Za du≈ºo kodu, za ma≈Ço tekstu (optimal >15%)',
            'fix': 'Dodaj wiƒôcej content lub upro≈õƒá HTML'
        })
    
    # 3. Readability (Flesch Reading Ease dla angielskiego, approx dla PL)
    if word_count > 50:
        try:
            # textstat domy≈õlnie dla EN, ale daje orientacyjnƒÖ warto≈õƒá
            reading_ease = textstat.flesch_reading_ease(text)
            
            # Interpretacja (im wy≈ºszy wynik tym ≈Çatwiejszy tekst)
            if reading_ease >= 60:
                read_score = 100
                read_label = '≈Åatwy'
            elif reading_ease >= 50:
                read_score = 80
                read_label = '≈öredni'
            else:
                read_score = 60
                read_label = 'Trudny'
            
            results['checks']['readability'] = {
                'value': f'{reading_ease:.0f} ({read_label})',
                'pass': reading_ease >= 50,
                'score': read_score
            }
            
            if reading_ease < 50:
                results['issues'].append({
                    'severity': 'recommendation',
                    'title': 'Tekst trudny w czytaniu',
                    'impact': 3,
                    'description': f'Readability score: {reading_ease:.0f} (im wy≈ºszy tym lepiej)',
                    'fix': 'U≈ºywaj kr√≥tszych zda≈Ñ i prostszego jƒôzyka'
                })
        except:
            results['checks']['readability'] = {'value': 'N/A', 'pass': True, 'score': 70}
    else:
        results['checks']['readability'] = {'value': 'Za ma≈Ço tekstu', 'pass': False, 'score': 0}
    
    # 4. Keyword density (przyk≈Çad - wykryj najpopularniejsze s≈Çowa)
    # Usu≈Ñ stop words (uproszczona wersja)
    stop_words = {'i', 'w', 'z', 'na', 'do', 'siƒô', 'to', '≈ºe', 'po', 'jest', 'byƒá', 'o', 'ale', 'dla', 'od', 'przez', 'oraz', 'jak', 'jako', 'wiƒôcej', 'te≈º', 'ju≈º', 'tylko', 'bardzo', 'by≈Ç', 'mo≈ºe', 'mo≈ºna', 'we', 'ze', 'and', 'the', 'a', 'an', 'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as', 'is', 'are', 'was', 'were'}
    
    clean_words = [word.lower() for word in words if len(word) > 3 and word.lower() not in stop_words]
    
    if clean_words:
        word_freq = Counter(clean_words)
        top_keywords = word_freq.most_common(5)
        top_word, top_count = top_keywords[0] if top_keywords else ('N/A', 0)
        
        keyword_density = (top_count / len(words) * 100) if words else 0
        
        results['checks']['keyword_density'] = {
            'value': f'{keyword_density:.1f}% ("{top_word}")',
            'pass': 1 <= keyword_density <= 3,
            'score': 100 if 1 <= keyword_density <= 3 else (50 if keyword_density < 5 else 0),
            'top_keywords': [(kw, cnt) for kw, cnt in top_keywords]
        }
        
        if keyword_density > 5:
            results['issues'].append({
                'severity': 'important',
                'title': f'Keyword stuffing: "{top_word}" ({keyword_density:.1f}%)',
                'impact': 7,
                'description': 'Zbyt wysoka gƒôsto≈õƒá s≈Çowa kluczowego (>5% = spam)',
                'fix': 'U≈ºyj synonimy i bardziej naturalnego jƒôzyka'
            })
    else:
        results['checks']['keyword_density'] = {'value': 'N/A', 'pass': True, 'score': 70}
    
    # 5. Paragraph analysis
    paragraphs = soup.find_all('p')
    long_paragraphs = [p for p in paragraphs if len(p.get_text().split()) > 150]
    
    results['checks']['paragraphs'] = {
        'value': f'{len(paragraphs)} paragraf√≥w',
        'pass': len(long_paragraphs) < len(paragraphs) * 0.3,
        'score': 100 if len(long_paragraphs) == 0 else 70
    }
    
    if len(long_paragraphs) > 0:
        results['issues'].append({
            'severity': 'recommendation',
            'title': f'{len(long_paragraphs)} d≈Çugich paragraf√≥w (>150 s≈Ç√≥w)',
            'impact': 3,
            'description': 'D≈Çugie paragrafy utrudniajƒÖ czytanie',
            'fix': 'Podziel d≈Çugie paragrafy na kr√≥tsze (50-100 s≈Ç√≥w)'
        })
    
    # Oblicz ≈õredni score
    scores = [check['score'] for check in results['checks'].values()]
    results['score'] = sum(scores) / len(scores) if scores else 0
    
    return results
```

### 7. **analyzers/pagespeed.py** - Google PSI API

```python
# analyzers/pagespeed.py
import requests
from config import GOOGLE_PSI_API_KEY, PSI_TIMEOUT

def analyze_pagespeed(url):
    """Analiza Core Web Vitals via Google PageSpeed Insights API"""
    results = {
        'score': 0,
        'checks': {},
        'issues': [],
        'mobile': {},
        'desktop': {}
    }
    
    # Sprawd≈∫ mobilnƒÖ wersjƒô
    try:
        mobile_data = fetch_psi_data(url, 'mobile')
        results['mobile'] = parse_psi_data(mobile_data)
        results['checks']['mobile_performance'] = {
            'value': f"{results['mobile']['performance_score']}/100",
            'pass': results['mobile']['performance_score'] >= 50,
            'score': results['mobile']['performance_score']
        }
        
        # Core Web Vitals issues
        cwv = results['mobile']['core_web_vitals']
        
        # LCP
        if cwv['lcp']['value'] > 2500:
            results['issues'].append({
                'severity': 'critical' if cwv['lcp']['value'] > 4000 else 'important',
                'title': f"Wolne LCP: {cwv['lcp']['value']/1000:.1f}s (mobile)",
                'impact': 9 if cwv['lcp']['value'] > 4000 else 7,
                'description': 'Largest Contentful Paint powinien byƒá <2.5s',
                'fix': 'Optymalizuj obrazy, u≈ºyj CDN, zminifikuj CSS/JS'
            })
        
        # CLS
        if cwv['cls']['value'] > 0.1:
            results['issues'].append({
                'severity': 'important' if cwv['cls']['value'] > 0.25 else 'recommendation',
                'title': f"Problemy z CLS: {cwv['cls']['value']:.3f} (mobile)",
                'impact': 7 if cwv['cls']['value'] > 0.25 else 5,
                'description': 'Cumulative Layout Shift powinien byƒá <0.1',
                'fix': 'Dodaj wymiary do obraz√≥w, zarezerwuj miejsce dla ads'
            })
        
        # FID/INP
        if cwv['fid']['value'] > 100:
            results['issues'].append({
                'severity': 'important',
                'title': f"Wolna interaktywno≈õƒá: {cwv['fid']['value']}ms (mobile)",
                'impact': 6,
                'description': 'First Input Delay powinien byƒá <100ms',
                'fix': 'Zmniejsz JavaScript, u≈ºyj code splitting, defer scripts'
            })
        
    except Exception as e:
        results['checks']['mobile_performance'] = {
            'value': 'Error',
            'pass': False,
            'score': 0,
            'error': str(e)
        }
        results['issues'].append({
            'severity': 'important',
            'title': 'Nie uda≈Ço siƒô pobraƒá danych PageSpeed',
            'impact': 5,
            'description': f'Error: {str(e)[:100]}',
            'fix': 'Sprawd≈∫ API key lub spr√≥buj ponownie'
        })
    
    # Oblicz score
    if results['mobile']:
        results['score'] = results['mobile']['performance_score']
    else:
        results['score'] = 0
    
    return results

def fetch_psi_data(url, strategy='mobile'):
    """Pobierz dane z PageSpeed Insights API"""
    endpoint = 'https://www.googleapis.com/pagespeedonline/v5/runPagespeed'
    
    params = {
        'url': url,
        'key': GOOGLE_PSI_API_KEY,
        'strategy': strategy,  # mobile lub desktop
        'category': 'performance'
    }
    
    response = requests.get(endpoint, params=params, timeout=PSI_TIMEOUT)
    response.raise_for_status()
    
    return response.json()

def parse_psi_data(data):
    """Parsuj dane z PSI API"""
    lighthouse = data.get('lighthouseResult', {})
    audits = lighthouse.get('audits', {})
    
    # Performance score
    perf_score = int(lighthouse.get('categories', {}).get('performance', {}).get('score', 0) * 100)
    
    # Core Web Vitals
    lcp_audit = audits.get('largest-contentful-paint', {})
    fid_audit = audits.get('max-potential-fid', {})
    cls_audit = audits.get('cumulative-layout-shift', {})
    
    return {
        'performance_score': perf_score,
        'core_web_vitals': {
            'lcp': {
                'value': lcp_audit.get('numericValue', 0),
                'displayValue': lcp_audit.get('displayValue', 'N/A')
            },
            'fid': {
                'value': fid_audit.get('numericValue', 0),
                'displayValue': fid_audit.get('displayValue', 'N/A')
            },
            'cls': {
                'value': cls_audit.get('numericValue', 0),
                'displayValue': cls_audit.get('displayValue', 'N/A')
            }
        },
        'opportunities': [
            {
                'title': opp.get('title', ''),
                'description': opp.get('description', ''),
                'savings': opp.get('numericValue', 0)
            }
            for key, opp in audits.items()
            if opp.get('details', {}).get('type') == 'opportunity'
        ][:5]  # Top 5 opportunities
    }
```

### 8. **audit_engine.py** - G≈Ç√≥wna logika

```python
# audit_engine.py
from utils import validate_url, fetch_url
from analyzers.technical import analyze_technical
from analyzers.onpage import analyze_onpage
from analyzers.indexing import analyze_indexing
from analyzers.content import analyze_content
from analyzers.pagespeed import analyze_pagespeed
from config import WEIGHTS

def run_audit(url):
    """Uruchom pe≈Çny audyt SEO"""
    
    # 1. Walidacja URL
    url = validate_url(url)
    if not url:
        return {'error': 'Invalid URL'}
    
    # 2. Pobierz stronƒô
    print(f"Fetching {url}...")
    page_data = fetch_url(url)
    
    if not page_data['success']:
        return {
            'error': 'Cannot fetch page',
            'details': page_data.get('error')
        }
    
    html_content = page_data['content']
    
    # 3. Uruchom wszystkie analizatory
    print("Running analyzers...")
    
    results = {
        'url': url,
        'timestamp': datetime.datetime.now().isoformat(),
        'categories': {}
    }
    
    # Technical
    print("  - Technical analysis...")
    results['categories']['technical'] = analyze_technical(url, page_data)
    
    # On-Page
    print("  - On-page analysis...")
    results['categories']['onpage'] = analyze_onpage(url, html_content)
    
    # Indexing
    print("  - Indexing analysis...")
    results['categories']['indexing'] = analyze_indexing(url, html_content)
    
    # Content
    print("  - Content analysis...")
    results['categories']['content'] = analyze_content(html_content)
    
    # PageSpeed (mo≈ºe zajƒÖƒá chwilƒô)
    print("  - PageSpeed analysis (Google API)...")
    try:
        results['categories']['pagespeed'] = analyze_pagespeed(url)
        # Merge PageSpeed score do technical
        results['categories']['technical']['core_web_vitals'] = results['categories']['pagespeed']['mobile']['core_web_vitals']
    except Exception as e:
        print(f"    Warning: PageSpeed failed: {e}")
        results['categories']['pagespeed'] = {'score': 0, 'error': str(e)}
    
    # 4. Oblicz finalny score
    final_score = calculate_final_score(results['categories'])
    results['final_score'] = final_score
    results['grade'] = get_grade(final_score)
    
    # 5. Agreguj wszystkie issues
    all_issues = []
    for category in results['categories'].values():
        all_issues.extend(category.get('issues', []))
    
    # Sortuj po impact (descending)
    all_issues.sort(key=lambda x: x['impact'], reverse=True)
    results['all_issues'] = all_issues
    
    # 6. Quick wins (high impact, ≈Çatwe)
    results['quick_wins'] = [
        issue for issue in all_issues
        if issue['impact'] >= 6 and issue['severity'] in ['important', 'recommendation']
    ][:5]
    
    print("Audit complete!")
    return results

def calculate_final_score(categories):
    """Oblicz finalny wynik z wagami"""
    score = 0
    score += categories['technical']['score'] * WEIGHTS['technical']
    score += categories['onpage']['score'] * WEIGHTS['onpage']
    score += categories['indexing']['score'] * WEIGHTS['indexing']
    score += categories['content']['score'] * WEIGHTS['content']
    
    # PageSpeed jako czƒô≈õƒá technical (ju≈º wliczone w technical)
    
    return round(score, 1)

def get_grade(score):
    """Okre≈õl ocenƒô literowƒÖ"""
    if score >= 90:
        return {'label': 'EXCELLENT', 'color': 'green', 'emoji': 'üü¢'}
    elif score >= 75:
        return {'label': 'GOOD', 'color': 'lightgreen', 'emoji': 'üü°'}
    elif score >= 60:
        return {'label': 'NEEDS IMPROVEMENT', 'color': 'yellow', 'emoji': 'üü†'}
    elif score >= 40:
        return {'label': 'POOR', 'color': 'orange', 'emoji': 'üî¥'}
    else:
        return {'label': 'CRITICAL', 'color': 'red', 'emoji': '‚õî'}

# Test
if __name__ == '__main__':
    import datetime
    
    test_url = "https://example.com"
    result = run_audit(test_url)
    
    print("\n=== AUDIT RESULTS ===")
    print(f"URL: {result['url']}")
    print(f"Final Score: {result['final_score']}/100 ({result['grade']['label']})")
    print(f"\nCategory Scores:")
    for cat_name, cat_data in result['categories'].items():
        print(f"  {cat_name}: {cat_data['score']:.1f}/100")
    
    print(f"\nTop Issues:")
    for issue in result['all_issues'][:5]:
        print(f"  [{issue['severity']}] {issue['title']} (Impact: {issue['impact']}/10)")
```

---

## üöÄ JAK URUCHOMIƒÜ

### Backend test (Python):

```bash
# 1. Zainstaluj dependencies
pip install requests beautifulsoup4 lxml textstat validators

# 2. Edytuj config.py - wstaw sw√≥j API key
# GOOGLE_PSI_API_KEY = "tw√≥j_klucz"

# 3. Uruchom test
python audit_engine.py
```

### Frontend (React Artifact):

W Claude.ai mo≈ºesz wygenerowaƒá pe≈ÇnƒÖ aplikacjƒô React jako artifact, kt√≥ra bƒôdzie wywo≈Çywa≈Ça backend przez API lub bƒôdzie dzia≈Ça≈Ça standalone z fetching bezpo≈õrednio z JavaScript (ograniczone CORS).

**Przyk≈Çad minimalistyczny - React artifact standalone:**

```javascript
// W artifact React mo≈ºna zrobiƒá uproszczonƒÖ wersjƒô kt√≥ra:
// 1. Pobiera HTML przez proxy (cors-anywhere)
// 2. Parsuje w przeglƒÖdarce (DOMParser)
// 3. Wywo≈Çuje Google PSI API bezpo≈õrednio
// 4. Pokazuje wyniki

// To da ci MVP dzia≈ÇajƒÖce w 100% w przeglƒÖdarce bez backendu!
```

---

## üí∞ KOSZTY I LIMITY

### Google PageSpeed Insights API:
- ‚úÖ **Darmowe**: 25,000 zapyta≈Ñ/dzie≈Ñ
- ‚úÖ Wystarczy na ~800 audyt√≥w dziennie
- ‚úÖ Nie wymaga karty kredytowej
- ‚úÖ Brak koszt√≥w ukrytych

### Pozosta≈Çe API/narzƒôdzia:
- ‚úÖ Wszystkie biblioteki Python: **darmowe** (open source)
- ‚úÖ Requests, BeautifulSoup, textstat: **darmowe**
- ‚úÖ Hosting: mo≈ºesz zrobiƒá static site (GitHub Pages, Netlify) - **darmowe**

**Total cost MVP: 0 PLN/miesiƒÖc**

---

## ‚úÖ CHECKLIST WDRO≈ªENIA MVP

- [ ] Uzyskaj Google PSI API key
- [ ] Zainstaluj Python dependencies
- [ ] Edytuj config.py (API key)
- [ ] Przetestuj ka≈ºdy analyzer osobno
- [ ] Uruchom audit_engine.py test
- [ ] Zbuduj React frontend (artifact)
- [ ] Po≈ÇƒÖcz frontend z backendem (API lub standalone)
- [ ] Test na 5-10 r√≥≈ºnych stronach
- [ ] Popraw b≈Çƒôdy/edge cases
- [ ] Deploy (opcjonalnie)

---

To jest kompletny plan implementacji MVP w 100% za darmo. Wszystko oparte na open source i darmowych API. Gotowe do wdro≈ºenia! üöÄ